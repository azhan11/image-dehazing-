{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, optim\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in c:\\users\\dell\\anaconda3\\envs\\azhan\\lib\\site-packages (0.9.12)\n",
      "Requirement already satisfied: safetensors in c:\\users\\dell\\anaconda3\\envs\\azhan\\lib\\site-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: torchvision in c:\\users\\dell\\anaconda3\\envs\\azhan\\lib\\site-packages (from timm) (0.14.1)\n",
      "Requirement already satisfied: torch>=1.7 in c:\\users\\dell\\appdata\\roaming\\python\\python37\\site-packages (from timm) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\dell\\anaconda3\\envs\\azhan\\lib\\site-packages (from timm) (0.16.4)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dell\\anaconda3\\envs\\azhan\\lib\\site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dell\\appdata\\roaming\\python\\python37\\site-packages (from torch>=1.7->timm) (4.7.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\dell\\anaconda3\\envs\\azhan\\lib\\site-packages (from huggingface-hub->timm) (6.7.0)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\envs\\azhan\\lib\\site-packages (from huggingface-hub->timm) (2.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\envs\\azhan\\lib\\site-packages (from huggingface-hub->timm) (3.12.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\dell\\anaconda3\\envs\\azhan\\lib\\site-packages (from huggingface-hub->timm) (4.66.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dell\\anaconda3\\envs\\azhan\\lib\\site-packages (from huggingface-hub->timm) (2023.1.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\dell\\anaconda3\\envs\\azhan\\lib\\site-packages (from huggingface-hub->timm) (22.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\appdata\\roaming\\python\\python37\\site-packages (from torchvision->timm) (1.21.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\dell\\appdata\\roaming\\python\\python37\\site-packages (from torchvision->timm) (9.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\envs\\azhan\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->timm) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dell\\anaconda3\\envs\\azhan\\lib\\site-packages (from importlib-metadata->huggingface-hub->timm) (3.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\roaming\\python\\python37\\site-packages (from requests->huggingface-hub->timm) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\roaming\\python\\python37\\site-packages (from requests->huggingface-hub->timm) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\roaming\\python\\python37\\site-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\roaming\\python\\python37\\site-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "from timm.models.layers import to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "class RLN(nn.Module):\n",
    "\tr\"\"\"Revised LayerNorm\"\"\"\n",
    "\tdef __init__(self, dim, eps=1e-5, detach_grad=False):\n",
    "\t\tsuper(RLN, self).__init__()\n",
    "\t\tself.eps = eps\n",
    "\t\tself.detach_grad = detach_grad\n",
    "\n",
    "\t\tself.weight = nn.Parameter(torch.ones((1, dim, 1, 1)))\n",
    "\t\tself.bias = nn.Parameter(torch.zeros((1, dim, 1, 1)))\n",
    "\n",
    "\t\tself.meta1 = nn.Conv2d(1, dim, 1)\n",
    "\t\tself.meta2 = nn.Conv2d(1, dim, 1)\n",
    "\n",
    "\t\ttrunc_normal_(self.meta1.weight, std=.02)\n",
    "\t\tnn.init.constant_(self.meta1.bias, 1)\n",
    "\n",
    "\t\ttrunc_normal_(self.meta2.weight, std=.02)\n",
    "\t\tnn.init.constant_(self.meta2.bias, 0)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tmean = torch.mean(input, dim=(1, 2, 3), keepdim=True)\n",
    "\t\tstd = torch.sqrt((input - mean).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
    "\n",
    "\t\tnormalized_input = (input - mean) / std\n",
    "\n",
    "\t\tif self.detach_grad:\n",
    "\t\t\trescale, rebias = self.meta1(std.detach()), self.meta2(mean.detach())\n",
    "\t\telse:\n",
    "\t\t\trescale, rebias = self.meta1(std), self.meta2(mean)\n",
    "\n",
    "\t\tout = normalized_input * self.weight + self.bias\n",
    "\t\treturn out, rescale, rebias\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "\tdef __init__(self, network_depth, in_features, hidden_features=None, out_features=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tout_features = out_features or in_features\n",
    "\t\thidden_features = hidden_features or in_features\n",
    "\n",
    "\t\tself.network_depth = network_depth\n",
    "\n",
    "\t\tself.mlp = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(in_features, hidden_features, 1),\n",
    "\t\t\tnn.ReLU(True),\n",
    "\t\t\tnn.Conv2d(hidden_features, out_features, 1)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.apply(self._init_weights)\n",
    "\n",
    "\tdef _init_weights(self, m):\n",
    "\t\tif isinstance(m, nn.Conv2d):\n",
    "\t\t\tgain = (8 * self.network_depth) ** (-1/4)\n",
    "\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n",
    "\t\t\tstd = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "\t\t\ttrunc_normal_(m.weight, std=std)\n",
    "\t\t\tif m.bias is not None:\n",
    "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.mlp(x)\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "\tB, H, W, C = x.shape\n",
    "\tx = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "\twindows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, C)\n",
    "\treturn windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "\tB = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "\tx = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "\tx = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "\treturn x\n",
    "\n",
    "\n",
    "def get_relative_positions(window_size):\n",
    "\tcoords_h = torch.arange(window_size)\n",
    "\tcoords_w = torch.arange(window_size)\n",
    "\n",
    "\tcoords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "\tcoords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "\trelative_positions = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "\n",
    "\trelative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "\trelative_positions_log  = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "\treturn relative_positions_log\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "\tdef __init__(self, dim, window_size, num_heads):\n",
    "\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim = dim\n",
    "\t\tself.window_size = window_size  # Wh, Ww\n",
    "\t\tself.num_heads = num_heads\n",
    "\t\thead_dim = dim // num_heads\n",
    "\t\tself.scale = head_dim ** -0.5\n",
    "\n",
    "\t\trelative_positions = get_relative_positions(self.window_size)\n",
    "\t\tself.register_buffer(\"relative_positions\", relative_positions)\n",
    "\t\tself.meta = nn.Sequential(\n",
    "\t\t\tnn.Linear(2, 256, bias=True),\n",
    "\t\t\tnn.ReLU(True),\n",
    "\t\t\tnn.Linear(256, num_heads, bias=True)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\tdef forward(self, qkv):\n",
    "\t\tB_, N, _ = qkv.shape\n",
    "\n",
    "\t\tqkv = qkv.reshape(B_, N, 3, self.num_heads, self.dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "\t\tq, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "\t\tq = q * self.scale\n",
    "\t\tattn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "\t\trelative_position_bias = self.meta(self.relative_positions)\n",
    "\t\trelative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "\t\tattn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "\t\tattn = self.softmax(attn)\n",
    "\n",
    "\t\tx = (attn @ v).transpose(1, 2).reshape(B_, N, self.dim)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\tdef __init__(self, network_depth, dim, num_heads, window_size, shift_size, use_attn=False, conv_type=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim = dim\n",
    "\t\tself.head_dim = int(dim // num_heads)\n",
    "\t\tself.num_heads = num_heads\n",
    "\n",
    "\t\tself.window_size = window_size\n",
    "\t\tself.shift_size = shift_size\n",
    "\n",
    "\t\tself.network_depth = network_depth\n",
    "\t\tself.use_attn = use_attn\n",
    "\t\tself.conv_type = conv_type\n",
    "\n",
    "\t\tif self.conv_type == 'Conv':\n",
    "\t\t\tself.conv = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "\t\t\t\tnn.ReLU(True),\n",
    "\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "\t\t\t)\n",
    "\n",
    "\t\tif self.conv_type == 'DWConv':\n",
    "\t\t\tself.conv = nn.Conv2d(dim, dim, kernel_size=5, padding=2, groups=dim, padding_mode='reflect')\n",
    "\n",
    "\t\tif self.conv_type == 'DWConv' or self.use_attn:\n",
    "\t\t\tself.V = nn.Conv2d(dim, dim, 1)\n",
    "\t\t\tself.proj = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "\t\tif self.use_attn:\n",
    "\t\t\tself.QK = nn.Conv2d(dim, dim * 2, 1)\n",
    "\t\t\tself.attn = WindowAttention(dim, window_size, num_heads)\n",
    "\n",
    "\t\tself.apply(self._init_weights)\n",
    "\n",
    "\tdef _init_weights(self, m):\n",
    "\t\tif isinstance(m, nn.Conv2d):\n",
    "\t\t\tw_shape = m.weight.shape\n",
    "\t\t\t\n",
    "\t\t\tif w_shape[0] == self.dim * 2:\t# QK\n",
    "\t\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n",
    "\t\t\t\tstd = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "\t\t\t\ttrunc_normal_(m.weight, std=std)\t\t\n",
    "\t\t\telse:\n",
    "\t\t\t\tgain = (8 * self.network_depth) ** (-1/4)\n",
    "\t\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n",
    "\t\t\t\tstd = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "\t\t\t\ttrunc_normal_(m.weight, std=std)\n",
    "\n",
    "\t\t\tif m.bias is not None:\n",
    "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
    "\n",
    "\tdef check_size(self, x, shift=False):\n",
    "\t\t_, _, h, w = x.size()\n",
    "\t\tmod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n",
    "\t\tmod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n",
    "\n",
    "\t\tif shift:\n",
    "\t\t\tx = F.pad(x, (self.shift_size, (self.window_size-self.shift_size+mod_pad_w) % self.window_size,\n",
    "\t\t\t\t\t\t  self.shift_size, (self.window_size-self.shift_size+mod_pad_h) % self.window_size), mode='reflect')\n",
    "\t\telse:\n",
    "\t\t\tx = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tB, C, H, W = X.shape\n",
    "\n",
    "\t\tif self.conv_type == 'DWConv' or self.use_attn:\n",
    "\t\t\tV = self.V(X)\n",
    "\n",
    "\t\tif self.use_attn:\n",
    "\t\t\tQK = self.QK(X)\n",
    "\t\t\tQKV = torch.cat([QK, V], dim=1)\n",
    "\n",
    "\t\t\t# shift\n",
    "\t\t\tshifted_QKV = self.check_size(QKV, self.shift_size > 0)\n",
    "\t\t\tHt, Wt = shifted_QKV.shape[2:]\n",
    "\n",
    "\t\t\t# partition windows\n",
    "\t\t\tshifted_QKV = shifted_QKV.permute(0, 2, 3, 1)\n",
    "\t\t\tqkv = window_partition(shifted_QKV, self.window_size)  # nW*B, window_size**2, C\n",
    "\n",
    "\t\t\tattn_windows = self.attn(qkv)\n",
    "\n",
    "\t\t\t# merge windows\n",
    "\t\t\tshifted_out = window_reverse(attn_windows, self.window_size, Ht, Wt)  # B H' W' C\n",
    "\n",
    "\t\t\t# reverse cyclic shift\n",
    "\t\t\tout = shifted_out[:, self.shift_size:(self.shift_size+H), self.shift_size:(self.shift_size+W), :]\n",
    "\t\t\tattn_out = out.permute(0, 3, 1, 2)\n",
    "\n",
    "\t\t\tif self.conv_type in ['Conv', 'DWConv']:\n",
    "\t\t\t\tconv_out = self.conv(V)\n",
    "\t\t\t\tout = self.proj(conv_out + attn_out)\n",
    "\t\t\telse:\n",
    "\t\t\t\tout = self.proj(attn_out)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tif self.conv_type == 'Conv':\n",
    "\t\t\t\tout = self.conv(X)\t\t\t\t# no attention and use conv, no projection\n",
    "\t\t\telif self.conv_type == 'DWConv':\n",
    "\t\t\t\tout = self.proj(self.conv(V))\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\tdef __init__(self, network_depth, dim, num_heads, mlp_ratio=4.,\n",
    "\t\t\t\t norm_layer=nn.LayerNorm, mlp_norm=False,\n",
    "\t\t\t\t window_size=8, shift_size=0, use_attn=True, conv_type=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.use_attn = use_attn\n",
    "\t\tself.mlp_norm = mlp_norm\n",
    "\n",
    "\t\tself.norm1 = norm_layer(dim) if use_attn else nn.Identity()\n",
    "\t\tself.attn = Attention(network_depth, dim, num_heads=num_heads, window_size=window_size,\n",
    "\t\t\t\t\t\t\t  shift_size=shift_size, use_attn=use_attn, conv_type=conv_type)\n",
    "\n",
    "\t\tself.norm2 = norm_layer(dim) if use_attn and mlp_norm else nn.Identity()\n",
    "\t\tself.mlp = Mlp(network_depth, dim, hidden_features=int(dim * mlp_ratio))\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tidentity = x\n",
    "\t\tif self.use_attn: x, rescale, rebias = self.norm1(x)\n",
    "\t\tx = self.attn(x)\n",
    "\t\tif self.use_attn: x = x * rescale + rebias\n",
    "\t\tx = identity + x\n",
    "\n",
    "\t\tidentity = x\n",
    "\t\tif self.use_attn and self.mlp_norm: x, rescale, rebias = self.norm2(x)\n",
    "\t\tx = self.mlp(x)\n",
    "\t\tif self.use_attn and self.mlp_norm: x = x * rescale + rebias\n",
    "\t\tx = identity + x\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "\tdef __init__(self, network_depth, dim, depth, num_heads, mlp_ratio=4.,\n",
    "\t\t\t\t norm_layer=nn.LayerNorm, window_size=8,\n",
    "\t\t\t\t attn_ratio=0., attn_loc='last', conv_type=None):\n",
    "\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim = dim\n",
    "\t\tself.depth = depth\n",
    "\n",
    "\t\tattn_depth = attn_ratio * depth\n",
    "\n",
    "\t\tif attn_loc == 'last':\n",
    "\t\t\tuse_attns = [i >= depth-attn_depth for i in range(depth)]\n",
    "\t\telif attn_loc == 'first':\n",
    "\t\t\tuse_attns = [i < attn_depth for i in range(depth)]\n",
    "\t\telif attn_loc == 'middle':\n",
    "\t\t\tuse_attns = [i >= (depth-attn_depth)//2 and i < (depth+attn_depth)//2 for i in range(depth)]\n",
    "\n",
    "\t\t# build blocks\n",
    "\t\tself.blocks = nn.ModuleList([\n",
    "\t\t\tTransformerBlock(network_depth=network_depth,\n",
    "\t\t\t\t\t\t\t dim=dim, \n",
    "\t\t\t\t\t\t\t num_heads=num_heads,\n",
    "\t\t\t\t\t\t\t mlp_ratio=mlp_ratio,\n",
    "\t\t\t\t\t\t\t norm_layer=norm_layer,\n",
    "\t\t\t\t\t\t\t window_size=window_size,\n",
    "\t\t\t\t\t\t\t shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "\t\t\t\t\t\t\t use_attn=use_attns[i], conv_type=conv_type)\n",
    "\t\t\tfor i in range(depth)])\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tfor blk in self.blocks:\n",
    "\t\t\tx = blk(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "\tdef __init__(self, patch_size=4, in_chans=3, embed_dim=96, kernel_size=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.in_chans = in_chans\n",
    "\t\tself.embed_dim = embed_dim\n",
    "\n",
    "\t\tif kernel_size is None:\n",
    "\t\t\tkernel_size = patch_size\n",
    "\n",
    "\t\tself.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "\t\t\t\t\t\t\t  padding=(kernel_size-patch_size+1)//2, padding_mode='reflect')\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.proj(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class PatchUnEmbed(nn.Module):\n",
    "\tdef __init__(self, patch_size=4, out_chans=3, embed_dim=96, kernel_size=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.out_chans = out_chans\n",
    "\t\tself.embed_dim = embed_dim\n",
    "\n",
    "\t\tif kernel_size is None:\n",
    "\t\t\tkernel_size = 1\n",
    "\n",
    "\t\tself.proj = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(embed_dim, out_chans*patch_size**2, kernel_size=kernel_size,\n",
    "\t\t\t\t\t  padding=kernel_size//2, padding_mode='reflect'),\n",
    "\t\t\tnn.PixelShuffle(patch_size)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.proj(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class SKFusion(nn.Module):\n",
    "\tdef __init__(self, dim, height=2, reduction=8):\n",
    "\t\tsuper(SKFusion, self).__init__()\n",
    "\t\t\n",
    "\t\tself.height = height\n",
    "\t\td = max(int(dim/reduction), 4)\n",
    "\t\t\n",
    "\t\tself.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\t\tself.mlp = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(dim, d, 1, bias=False), \n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(d, dim*height, 1, bias=False)\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\tdef forward(self, in_feats):\n",
    "\t\tB, C, H, W = in_feats[0].shape\n",
    "\t\t\n",
    "\t\tin_feats = torch.cat(in_feats, dim=1)\n",
    "\t\tin_feats = in_feats.view(B, self.height, C, H, W)\n",
    "\t\t\n",
    "\t\tfeats_sum = torch.sum(in_feats, dim=1)\n",
    "\t\tattn = self.mlp(self.avg_pool(feats_sum))\n",
    "\t\tattn = self.softmax(attn.view(B, self.height, C, 1, 1))\n",
    "\n",
    "\t\tout = torch.sum(in_feats*attn, dim=1)\n",
    "\t\treturn out      \n",
    "\n",
    "\n",
    "class DehazeFormer(nn.Module):\n",
    "\tdef __init__(self, in_chans=3, out_chans=4, window_size=8,\n",
    "\t\t\t\t embed_dims=[24, 48, 96, 48, 24],\n",
    "\t\t\t\t mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\t\t\t depths=[16, 16, 16, 8, 8],\n",
    "\t\t\t\t num_heads=[2, 4, 6, 1, 1],\n",
    "\t\t\t\t attn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\t\t\t conv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "\t\t\t\t norm_layer=[RLN, RLN, RLN, RLN, RLN]):\n",
    "\t\tsuper(DehazeFormer, self).__init__()\n",
    "\n",
    "\t\t# setting\n",
    "\t\tself.patch_size = 4\n",
    "\t\tself.window_size = window_size\n",
    "\t\tself.mlp_ratios = mlp_ratios\n",
    "\n",
    "\t\t# split image into non-overlapping patches\n",
    "\t\tself.patch_embed = PatchEmbed(\n",
    "\t\t\tpatch_size=1, in_chans=in_chans, embed_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "\t\t# backbone\n",
    "\t\tself.layer1 = BasicLayer(network_depth=sum(depths), dim=embed_dims[0], depth=depths[0],\n",
    "\t\t\t\t\t   \t\t\t num_heads=num_heads[0], mlp_ratio=mlp_ratios[0],\n",
    "\t\t\t\t\t   \t\t\t norm_layer=norm_layer[0], window_size=window_size,\n",
    "\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio[0], attn_loc='last', conv_type=conv_type[0])\n",
    "\n",
    "\t\tself.patch_merge1 = PatchEmbed(\n",
    "\t\t\tpatch_size=2, in_chans=embed_dims[0], embed_dim=embed_dims[1])\n",
    "\n",
    "\t\tself.skip1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "\n",
    "\t\tself.layer2 = BasicLayer(network_depth=sum(depths), dim=embed_dims[1], depth=depths[1],\n",
    "\t\t\t\t\t\t\t\t num_heads=num_heads[1], mlp_ratio=mlp_ratios[1],\n",
    "\t\t\t\t\t\t\t\t norm_layer=norm_layer[1], window_size=window_size,\n",
    "\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[1], attn_loc='last', conv_type=conv_type[1])\n",
    "\n",
    "\t\tself.patch_merge2 = PatchEmbed(\n",
    "\t\t\tpatch_size=2, in_chans=embed_dims[1], embed_dim=embed_dims[2])\n",
    "\n",
    "\t\tself.skip2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "\n",
    "\t\tself.layer3 = BasicLayer(network_depth=sum(depths), dim=embed_dims[2], depth=depths[2],\n",
    "\t\t\t\t\t\t\t\t num_heads=num_heads[2], mlp_ratio=mlp_ratios[2],\n",
    "\t\t\t\t\t\t\t\t norm_layer=norm_layer[2], window_size=window_size,\n",
    "\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[2], attn_loc='last', conv_type=conv_type[2])\n",
    "\n",
    "\t\tself.patch_split1 = PatchUnEmbed(\n",
    "\t\t\tpatch_size=2, out_chans=embed_dims[3], embed_dim=embed_dims[2])\n",
    "\n",
    "\t\tassert embed_dims[1] == embed_dims[3]\n",
    "\t\tself.fusion1 = SKFusion(embed_dims[3])\n",
    "\n",
    "\t\tself.layer4 = BasicLayer(network_depth=sum(depths), dim=embed_dims[3], depth=depths[3],\n",
    "\t\t\t\t\t\t\t\t num_heads=num_heads[3], mlp_ratio=mlp_ratios[3],\n",
    "\t\t\t\t\t\t\t\t norm_layer=norm_layer[3], window_size=window_size,\n",
    "\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[3], attn_loc='last', conv_type=conv_type[3])\n",
    "\n",
    "\t\tself.patch_split2 = PatchUnEmbed(\n",
    "\t\t\tpatch_size=2, out_chans=embed_dims[4], embed_dim=embed_dims[3])\n",
    "\n",
    "\t\tassert embed_dims[0] == embed_dims[4]\n",
    "\t\tself.fusion2 = SKFusion(embed_dims[4])\t\t\t\n",
    "\n",
    "\t\tself.layer5 = BasicLayer(network_depth=sum(depths), dim=embed_dims[4], depth=depths[4],\n",
    "\t\t\t\t\t   \t\t\t num_heads=num_heads[4], mlp_ratio=mlp_ratios[4],\n",
    "\t\t\t\t\t   \t\t\t norm_layer=norm_layer[4], window_size=window_size,\n",
    "\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio[4], attn_loc='last', conv_type=conv_type[4])\n",
    "\n",
    "\t\t# merge non-overlapping patches into image\n",
    "\t\tself.patch_unembed = PatchUnEmbed(\n",
    "\t\t\tpatch_size=1, out_chans=out_chans, embed_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "\n",
    "\tdef check_image_size(self, x):\n",
    "\t\t# NOTE: for I2I test\n",
    "\t\t_, _, h, w = x.size()\n",
    "\t\tmod_pad_h = (self.patch_size - h % self.patch_size) % self.patch_size\n",
    "\t\tmod_pad_w = (self.patch_size - w % self.patch_size) % self.patch_size\n",
    "\t\tx = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward_features(self, x):\n",
    "\t\tx = self.patch_embed(x)\n",
    "\t\tx = self.layer1(x)\n",
    "\t\tskip1 = x\n",
    "\n",
    "\t\tx = self.patch_merge1(x)\n",
    "\t\tx = self.layer2(x)\n",
    "\t\tskip2 = x\n",
    "\n",
    "\t\tx = self.patch_merge2(x)\n",
    "\t\tx = self.layer3(x)\n",
    "\t\tx = self.patch_split1(x)\n",
    "\n",
    "\t\tx = self.fusion1([x, self.skip2(skip2)]) + x\n",
    "\t\tx = self.layer4(x)\n",
    "\t\tx = self.patch_split2(x)\n",
    "\n",
    "\t\tx = self.fusion2([x, self.skip1(skip1)]) + x\n",
    "\t\tx = self.layer5(x)\n",
    "\t\tx = self.patch_unembed(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tH, W = x.shape[2:]\n",
    "\t\tx = self.check_image_size(x)\n",
    "\n",
    "\t\tfeat = self.forward_features(x)\n",
    "\t\tK, B = torch.split(feat, (1, 3), dim=1)\n",
    "\n",
    "\t\tx = K * x - B + x\n",
    "\t\tx = x[:, :, :H, :W]\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "def dehazeformer_t():\n",
    "    return DehazeFormer(\n",
    "\t\tembed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[4, 4, 4, 2, 2],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[0, 1/2, 1, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_s():\n",
    "    return DehazeFormer(\n",
    "\t\tembed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[8, 8, 8, 4, 4],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_b():\n",
    "    return DehazeFormer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[16, 16, 16, 8, 8],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_d():\n",
    "    return DehazeFormer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[32, 32, 32, 16, 16],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_w():\n",
    "    return DehazeFormer(\n",
    "        embed_dims=[48, 96, 192, 96, 48],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[16, 16, 16, 8, 8],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_m():\n",
    "    return DehazeFormer(\n",
    "\t\tembed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[12, 12, 12, 6, 6],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['Conv', 'Conv', 'Conv', 'Conv', 'Conv'])\n",
    "\n",
    "\n",
    "def dehazeformer_l():\n",
    "    return DehazeFormer(\n",
    "\t\tembed_dims=[48, 96, 192, 96, 48],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[16, 16, 16, 12, 12],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['Conv', 'Conv', 'Conv', 'Conv', 'Conv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers):\n",
    "        super(GeminiNet, self).__init__()\n",
    "\n",
    "        # Reduce number of channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, padding=1)\n",
    "\n",
    "        # Transformer blocks with reduced complexity\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(self._create_transformer_block(16))\n",
    "\n",
    "        # Patch Un-embedding with simpler upsampling\n",
    "        self.conv2 = nn.Conv2d(16, out_channels, kernel_size=3, padding=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2)  # Replace PixelShuffle\n",
    "\n",
    "    def _create_transformer_block(self, channels):\n",
    "        # Simplified Attention with local window\n",
    "        self.attn = nn.MultiheadAttention(channels, num_heads=1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels * 2, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels * 2, channels, kernel_size=1),\n",
    "        )\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(channels), self.attn, nn.LayerNorm(channels), self.mlp\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # Loop through simplified transformer blocks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.upsample(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixStructureBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.BatchNorm2d(dim)\n",
    "        self.norm2 = nn.BatchNorm2d(dim)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(dim, dim, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(dim, dim, kernel_size=5, padding=2, padding_mode='reflect')\n",
    "        self.conv3_19 = nn.Conv2d(dim, dim, kernel_size=7, padding=9, groups=dim, dilation=3, padding_mode='reflect')\n",
    "        self.conv3_13 = nn.Conv2d(dim, dim, kernel_size=5, padding=6, groups=dim, dilation=3, padding_mode='reflect')\n",
    "        self.conv3_7 = nn.Conv2d(dim, dim, kernel_size=3, padding=3, groups=dim, dilation=3, padding_mode='reflect')\n",
    "\n",
    "        # Simple Channel Attention\n",
    "        self.Wv = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, 1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=3 // 2, groups=dim, padding_mode='reflect')\n",
    "        )\n",
    "        self.Wg = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(dim, dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Channel Attention\n",
    "        self.ca = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(dim, dim, 1, padding=0, bias=True),\n",
    "            nn.GELU(),\n",
    "            # nn.ReLU(True),\n",
    "            nn.Conv2d(dim, dim, 1, padding=0, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Pixel Attention\n",
    "        self.pa = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim // 8, 1, padding=0, bias=True),\n",
    "            nn.GELU(),\n",
    "            # nn.ReLU(True),\n",
    "            nn.Conv2d(dim // 8, 1, 1, padding=0, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(dim * 3, dim * 4, 1),\n",
    "            nn.GELU(),\n",
    "            # nn.ReLU(True),\n",
    "            nn.Conv2d(dim * 4, dim, 1)\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv2d(dim * 3, dim * 4, 1),\n",
    "            nn.GELU(),\n",
    "            # nn.ReLU(True),\n",
    "            nn.Conv2d(dim * 4, dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.cat([self.conv3_19(x), self.conv3_13(x), self.conv3_7(x)], dim=1)\n",
    "        x = self.mlp(x)\n",
    "        x = identity + x\n",
    "\n",
    "        identity = x\n",
    "        x = self.norm2(x)\n",
    "        x = torch.cat([self.Wv(x) * self.Wg(x), self.ca(x) * x, self.pa(x) * x], dim=1)\n",
    "        x = self.mlp2(x)\n",
    "        x = identity + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayerMix(nn.Module):\n",
    "    def __init__(self, dim, depth):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [MixStructureBlock(dim=dim) for i in range(depth)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, kernel_size=None):\n",
    "        super().__init__()\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = patch_size\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "                              padding=(kernel_size - patch_size + 1) // 2, padding_mode='reflect')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchUnEmbed(nn.Module):\n",
    "    def __init__(self, patch_size=4, out_chans=3, embed_dim=96, kernel_size=None):\n",
    "        super().__init__()\n",
    "        self.out_chans = out_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 1\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, out_chans * patch_size ** 2, kernel_size=kernel_size,\n",
    "                      padding=kernel_size // 2, padding_mode='reflect'),\n",
    "            nn.PixelShuffle(patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SKFusion(nn.Module):\n",
    "    def __init__(self, dim, height=2, reduction=8):\n",
    "        super(SKFusion, self).__init__()\n",
    "\n",
    "        self.height = height\n",
    "        d = max(int(dim / reduction), 4)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(dim, d, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(d, dim * height, 1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, in_feats):\n",
    "        B, C, H, W = in_feats[0].shape\n",
    "\n",
    "        in_feats = torch.cat(in_feats, dim=1)\n",
    "        in_feats = in_feats.view(B, self.height, C, H, W)\n",
    "\n",
    "        feats_sum = torch.sum(in_feats, dim=1)\n",
    "        attn = self.mlp(self.avg_pool(feats_sum))\n",
    "        attn = self.softmax(attn.view(B, self.height, C, 1, 1))\n",
    "\n",
    "        out = torch.sum(in_feats * attn, dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MixDehazeNet(nn.Module):\n",
    "    def __init__(self, in_chans=3, out_chans=4,\n",
    "                 embed_dims=[24, 48, 96, 48, 24],\n",
    "                 depths=[1, 1, 2, 1, 1]):\n",
    "        super(MixDehazeNet, self).__init__()\n",
    "\n",
    "        # setting\n",
    "        self.patch_size = 4\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=1, in_chans=in_chans, embed_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "        # backbone\n",
    "        self.layer1 = BasicLayerMix(dim=embed_dims[0], depth=depths[0])\n",
    "\n",
    "        self.patch_merge1 = PatchEmbed(\n",
    "            patch_size=2, in_chans=embed_dims[0], embed_dim=embed_dims[1], kernel_size=3)\n",
    "\n",
    "        self.skip1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "\n",
    "        self.layer2 = BasicLayerMix(dim=embed_dims[1], depth=depths[1])\n",
    "\n",
    "        self.patch_merge2 = PatchEmbed(\n",
    "            patch_size=2, in_chans=embed_dims[1], embed_dim=embed_dims[2], kernel_size=3)\n",
    "\n",
    "        self.skip2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "\n",
    "        self.layer3 = BasicLayerMix(dim=embed_dims[2], depth=depths[2])\n",
    "\n",
    "        self.patch_split1 = PatchUnEmbed(\n",
    "            patch_size=2, out_chans=embed_dims[3], embed_dim=embed_dims[2])\n",
    "\n",
    "        assert embed_dims[1] == embed_dims[3]\n",
    "        self.fusion1 = SKFusion(embed_dims[3])\n",
    "\n",
    "        self.layer4 = BasicLayerMix(dim=embed_dims[3], depth=depths[3])\n",
    "\n",
    "        self.patch_split2 = PatchUnEmbed(\n",
    "            patch_size=2, out_chans=embed_dims[4], embed_dim=embed_dims[3])\n",
    "\n",
    "        assert embed_dims[0] == embed_dims[4]\n",
    "        self.fusion2 = SKFusion(embed_dims[4])\n",
    "\n",
    "        self.layer5 = BasicLayerMix(dim=embed_dims[4], depth=depths[4])\n",
    "\n",
    "        # merge non-overlapping patches into image\n",
    "        self.patch_unembed = PatchUnEmbed(\n",
    "            patch_size=1, out_chans=out_chans, embed_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "    def check_image_size(self, x):\n",
    "        # NOTE: for I2I test\n",
    "        _, _, h, w = x.size()\n",
    "        mod_pad_h = (self.patch_size - h % self.patch_size) % self.patch_size\n",
    "        mod_pad_w = (self.patch_size - w % self.patch_size) % self.patch_size\n",
    "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.layer1(x)\n",
    "        skip1 = x\n",
    "\n",
    "        x = self.patch_merge1(x)\n",
    "        x = self.layer2(x)\n",
    "        skip2 = x\n",
    "\n",
    "        x = self.patch_merge2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.patch_split1(x)\n",
    "\n",
    "        x = self.fusion1([x, self.skip2(skip2)]) + x\n",
    "        x = self.layer4(x)\n",
    "        x = self.patch_split2(x)\n",
    "\n",
    "        x = self.fusion2([x, self.skip1(skip1)]) + x\n",
    "        x = self.layer5(x)\n",
    "        x = self.patch_unembed(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = x.shape[2:]\n",
    "        x = self.check_image_size(x)\n",
    "\n",
    "        feat = self.forward_features(x)\n",
    "        # 2022/11/26\n",
    "        K, B = torch.split(feat, (1, 3), dim=1)\n",
    "\n",
    "        x = K * x - B + x\n",
    "        x = x[:, :, :H, :W]\n",
    "        return x\n",
    "\n",
    "\n",
    "def MixDehazeNet_t():\n",
    "    return MixDehazeNet(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        depths=[1, 1, 2, 1, 1])\n",
    "\n",
    "def MixDehazeNet_s():\n",
    "    return MixDehazeNet(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        depths=[2, 2, 4, 2, 2])\n",
    "\n",
    "def MixDehazeNet_b():\n",
    "    return MixDehazeNet(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        depths=[4, 4, 8, 4, 4])\n",
    "\n",
    "def MixDehazeNet_l():\n",
    "    return MixDehazeNet(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        depths=[8, 8, 16, 8, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, bias=True, norm=False, relu=True, transpose=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        if bias and norm:\n",
    "            bias = False\n",
    "\n",
    "        padding = kernel_size // 2\n",
    "        layers = list()\n",
    "        if transpose:\n",
    "            padding = kernel_size // 2 -1\n",
    "            layers.append(nn.ConvTranspose2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))\n",
    "        else:\n",
    "            layers.append(\n",
    "                nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))\n",
    "        if norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channel))\n",
    "        if relu:\n",
    "            layers.append(nn.GELU())\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, filter=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = BasicConv(in_channel, out_channel, kernel_size=3, stride=1, relu=True)\n",
    "        self.conv2 = BasicConv(out_channel, out_channel, kernel_size=3, stride=1, relu=False)\n",
    "        self.dyna_ch = depth_channel_att(in_channel) if filter else nn.Identity()\n",
    "        self.sfconv = SFconv(in_channel) if filter else nn.Identity()\n",
    "    \n",
    "        self.proj = nn.Conv2d(out_channel, out_channel, 3, 1, 1, groups=out_channel)\n",
    "        self.proj_act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.conv1(x)\n",
    "\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_act(out)\n",
    "        out = self.dyna_ch(out)\n",
    "\n",
    "        out = self.sfconv(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        return out + x\n",
    "\n",
    "class depth_channel_att(nn.Module):\n",
    "    def __init__(self, dim, kernel=3) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.kernel = (1, kernel)\n",
    "        pad_r = pad_l = kernel // 2\n",
    "        self.pad = nn.ReflectionPad2d((pad_r, pad_l, 0, 0))\n",
    "        self.conv = nn.Conv2d(dim, kernel*dim, kernel_size=1, stride=1, bias=False, groups=1)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.filter_act = nn.Tanh()\n",
    "        self.filter_bn = nn.BatchNorm2d(kernel*dim)\n",
    "        self.gamma = nn.Parameter(torch.zeros(dim,1,1))\n",
    "        self.beta = nn.Parameter(torch.ones(dim,1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        filter = self.filter_bn(self.conv(self.gap(x))) \n",
    "        filter = self.filter_act(filter) \n",
    "        b, c, h, w = filter.shape\n",
    "        filter = filter.view(b, self.kernel[1], c//self.kernel[1], h*w).permute(0, 1, 3, 2).contiguous()\n",
    "        B, C, H, W = x.shape\n",
    "        out = x.permute(0, 2, 3, 1).view(B, H*W, C).unsqueeze(1)\n",
    "        out = F.unfold(self.pad(out), kernel_size=self.kernel, stride=1) \n",
    "        out = out.view(B, self.kernel[1], H*W, -1)\n",
    "        out = torch.sum(out * filter, dim=1, keepdim=True).permute(0,3,1,2).reshape(B,C,H,W)\n",
    "\n",
    "        return out * self.gamma + x * self.beta\n",
    "\n",
    "class SFconv(nn.Module):\n",
    "    def __init__(self, features, M=4, r=2, L=32) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        d = max(int(features/r), L)\n",
    "        self.features = features\n",
    "        self.convs = nn.ModuleList([])\n",
    "\n",
    "        self.convh = nn.Sequential(\n",
    "            nn.Conv2d(features, features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GELU()\n",
    "            )\n",
    "\n",
    "        self.convm = nn.Sequential(\n",
    "            nn.Conv2d(features, features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.convl = nn.Sequential(\n",
    "            nn.Conv2d(features, features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.convll = nn.Sequential(\n",
    "            nn.Conv2d(features, features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Conv2d(features, d, 1, 1, 0)\n",
    "        self.fcs = nn.ModuleList([])\n",
    "        for i in range(M):\n",
    "            self.fcs.append(\n",
    "                nn.Conv2d(d, features, 1, 1, 0)\n",
    "            )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.out = nn.Conv2d(features, features, 1, 1, 0)\n",
    "        self.gamma = nn.Parameter(torch.zeros((1,features,1,1)), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lowlow = self.convll(x)\n",
    "        low = self.convl(lowlow)\n",
    "        middle = self.convm(low)\n",
    "        high = self.convh(middle)\n",
    "        emerge = low+middle+high+lowlow\n",
    "        emerge = self.gap(emerge)\n",
    "\n",
    "        fea_z = self.fc(emerge)\n",
    "\n",
    "        high_att = self.fcs[0](fea_z)\n",
    "        middle_att = self.fcs[1](fea_z)\n",
    "        low_att = self.fcs[2](fea_z)\n",
    "        lowlow_att = self.fcs[3](fea_z)\n",
    "\n",
    "        attention_vectors = torch.cat([high_att, middle_att, low_att, lowlow_att], dim=1)\n",
    "\n",
    "        attention_vectors = self.softmax(attention_vectors)\n",
    "        high_att, middle_att, low_att, lowlow_att = torch.chunk(attention_vectors, 4, dim=1)\n",
    "\n",
    "        fea_high = high * high_att\n",
    "        fea_middle = middle * middle_att\n",
    "        fea_low = low * low_att\n",
    "        fea_lowlow = lowlow * lowlow_att\n",
    "        out = self.out(fea_high + fea_middle + fea_low + fea_lowlow) \n",
    "        return out * self.gamma + x\n",
    "    \n",
    "class EBlock(nn.Module):\n",
    "    def __init__(self, out_channel, num_res=8):\n",
    "        super(EBlock, self).__init__()\n",
    "\n",
    "        layers = [ResBlock(out_channel, out_channel) for _ in range(num_res-1)]\n",
    "        layers.append(ResBlock(out_channel, out_channel, filter=True))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class DBlock(nn.Module):\n",
    "    def __init__(self, channel, num_res=8):\n",
    "        super(DBlock, self).__init__()\n",
    "\n",
    "        layers = [ResBlock(channel, channel) for _ in range(num_res-1)]\n",
    "        layers.append(ResBlock(channel, channel, filter=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class SCM(nn.Module):\n",
    "    def __init__(self, out_plane):\n",
    "        super(SCM, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            BasicConv(3, out_plane//4, kernel_size=3, stride=1, relu=True),\n",
    "            BasicConv(out_plane // 4, out_plane // 2, kernel_size=1, stride=1, relu=True),\n",
    "            BasicConv(out_plane // 2, out_plane // 2, kernel_size=3, stride=1, relu=True),\n",
    "            BasicConv(out_plane // 2, out_plane, kernel_size=1, stride=1, relu=False),\n",
    "            nn.InstanceNorm2d(out_plane, affine=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FAM(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(FAM, self).__init__()\n",
    "        self.merge = BasicConv(channel*2, channel, kernel_size=3, stride=1, relu=False)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return self.merge(torch.cat([x1, x2], dim=1))\n",
    "\n",
    "class ChaIR(nn.Module):\n",
    "    def __init__(self, num_res=16):\n",
    "        super(ChaIR, self).__init__()\n",
    "\n",
    "        base_channel = 32\n",
    "\n",
    "        self.Encoder = nn.ModuleList([\n",
    "            EBlock(base_channel, num_res),\n",
    "            EBlock(base_channel*2, num_res),\n",
    "            EBlock(base_channel*4, num_res),\n",
    "        ])\n",
    "\n",
    "        self.feat_extract = nn.ModuleList([\n",
    "            BasicConv(3, base_channel, kernel_size=3, relu=True, stride=1),\n",
    "            BasicConv(base_channel, base_channel*2, kernel_size=3, relu=True, stride=2),\n",
    "            BasicConv(base_channel*2, base_channel*4, kernel_size=3, relu=True, stride=2),\n",
    "            BasicConv(base_channel*4, base_channel*2, kernel_size=4, relu=True, stride=2, transpose=True),\n",
    "            BasicConv(base_channel*2, base_channel, kernel_size=4, relu=True, stride=2, transpose=True),\n",
    "            BasicConv(base_channel, 3, kernel_size=3, relu=False, stride=1)\n",
    "        ])\n",
    "\n",
    "        self.Decoder = nn.ModuleList([\n",
    "            DBlock(base_channel * 4, num_res),\n",
    "            DBlock(base_channel * 2, num_res),\n",
    "            DBlock(base_channel, num_res)\n",
    "        ])\n",
    "\n",
    "        self.Convs = nn.ModuleList([\n",
    "            BasicConv(base_channel * 4, base_channel * 2, kernel_size=1, relu=True, stride=1),\n",
    "            BasicConv(base_channel * 2, base_channel, kernel_size=1, relu=True, stride=1),\n",
    "        ])\n",
    "\n",
    "        self.ConvsOut = nn.ModuleList(\n",
    "            [\n",
    "                BasicConv(base_channel * 4, 3, kernel_size=3, relu=False, stride=1),\n",
    "                BasicConv(base_channel * 2, 3, kernel_size=3, relu=False, stride=1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.FAM1 = FAM(base_channel * 4)\n",
    "        self.SCM1 = SCM(base_channel * 4)\n",
    "        self.FAM2 = FAM(base_channel * 2)\n",
    "        self.SCM2 = SCM(base_channel * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_2 = F.interpolate(x, scale_factor=0.5)\n",
    "        x_4 = F.interpolate(x_2, scale_factor=0.5)\n",
    "        z2 = self.SCM2(x_2)\n",
    "        z4 = self.SCM1(x_4)\n",
    "\n",
    "        outputs = list()\n",
    "        # 256\n",
    "        x_ = self.feat_extract[0](x)\n",
    "        res1 = self.Encoder[0](x_)\n",
    "        # 128\n",
    "        z = self.feat_extract[1](res1)\n",
    "        z = self.FAM2(z, z2)\n",
    "        res2 = self.Encoder[1](z)\n",
    "        # 64\n",
    "        z = self.feat_extract[2](res2)\n",
    "        z = self.FAM1(z, z4)\n",
    "        z = self.Encoder[2](z)\n",
    "\n",
    "        z = self.Decoder[0](z)\n",
    "        z_ = self.ConvsOut[0](z)\n",
    "        # 128\n",
    "        z = self.feat_extract[3](z)\n",
    "        outputs.append(z_+x_4)\n",
    "\n",
    "        z = torch.cat([z, res2], dim=1)\n",
    "        z = self.Convs[0](z)\n",
    "        z = self.Decoder[1](z)\n",
    "        z_ = self.ConvsOut[1](z)\n",
    "        # 256\n",
    "        z = self.feat_extract[4](z)\n",
    "        outputs.append(z_+x_2)\n",
    "\n",
    "        z = torch.cat([z, res1], dim=1)\n",
    "        z = self.Convs[1](z)\n",
    "        z = self.Decoder[2](z)\n",
    "        z = self.feat_extract[5](z)\n",
    "        outputs.append(z+x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def build_net():\n",
    "    return ChaIR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_conv(in_channels, out_channels, kernel_size, bias=True):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size,padding=(kernel_size//2), bias=bias)\n",
    "    \n",
    "class PALayer(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(PALayer, self).__init__()\n",
    "        self.pa = nn.Sequential(\n",
    "                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // 8, 1, 1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.pa(x)\n",
    "        return x * y\n",
    "\n",
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(CALayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.ca = nn.Sequential(\n",
    "                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.ca(y)\n",
    "        return x * y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, conv, dim, kernel_size,):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1=conv(dim, dim, kernel_size, bias=True)\n",
    "        self.act1=nn.ReLU(inplace=True)\n",
    "        self.conv2=conv(dim,dim,kernel_size,bias=True)\n",
    "        self.calayer=CALayer(dim)\n",
    "        self.palayer=PALayer(dim)\n",
    "    def forward(self, x):\n",
    "        res=self.act1(self.conv1(x))\n",
    "        res=res+x \n",
    "        res=self.conv2(res)\n",
    "        res=self.calayer(res)\n",
    "        res=self.palayer(res)\n",
    "        res += x \n",
    "        return res\n",
    "class Group(nn.Module):\n",
    "    def __init__(self, conv, dim, kernel_size, blocks):\n",
    "        super(Group, self).__init__()\n",
    "        modules = [ Block(conv, dim, kernel_size)  for _ in range(blocks)]\n",
    "        modules.append(conv(dim, dim, kernel_size))\n",
    "        self.gp = nn.Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        res = self.gp(x)\n",
    "        res += x\n",
    "        return res\n",
    "\n",
    "class FFA(nn.Module):\n",
    "    def __init__(self,gps,blocks,conv=default_conv):\n",
    "        super(FFA, self).__init__()\n",
    "        self.gps=gps\n",
    "        self.dim=64\n",
    "        kernel_size=3\n",
    "        pre_process = [conv(3, self.dim, kernel_size)]\n",
    "        assert self.gps==3\n",
    "        self.g1= Group(conv, self.dim, kernel_size,blocks=blocks)\n",
    "        self.g2= Group(conv, self.dim, kernel_size,blocks=blocks)\n",
    "        self.g3= Group(conv, self.dim, kernel_size,blocks=blocks)\n",
    "        self.ca=nn.Sequential(*[\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(self.dim*self.gps,self.dim//16,1,padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(self.dim//16, self.dim*self.gps, 1, padding=0, bias=True),\n",
    "            nn.Sigmoid()\n",
    "            ])\n",
    "        self.palayer=PALayer(self.dim)\n",
    "\n",
    "        post_precess = [\n",
    "            conv(self.dim, self.dim, kernel_size),\n",
    "            conv(self.dim, 3, kernel_size)]\n",
    "\n",
    "        self.pre = nn.Sequential(*pre_process)\n",
    "        self.post = nn.Sequential(*post_precess)\n",
    "\n",
    "    def forward(self, x1):\n",
    "        x = self.pre(x1)\n",
    "        res1=self.g1(x)\n",
    "        res2=self.g2(res1)\n",
    "        res3=self.g3(res2)\n",
    "        w=self.ca(torch.cat([res1,res2,res3],dim=1))\n",
    "        w=w.view(-1,self.gps,self.dim)[:,:,:,None,None]\n",
    "        out=w[:,0,::]*res1+w[:,1,::]*res2+w[:,2,::]*res3\n",
    "        out=self.palayer(out)\n",
    "        x=self.post(out)\n",
    "        return x + x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(raw_image, model):\n",
    "\timage = np.array(raw_image, np.float32) / 255. * 2 - 1\n",
    "\timage = torch.from_numpy(image).to(device)\n",
    "\timage = image.permute((2, 0, 1)).unsqueeze(0)\n",
    "\n",
    "\tmodel.eval()\n",
    "\twith torch.inference_mode():\n",
    "\t\toutput = model(image).clamp_(-1, 1)[0] * 0.5 + 0.5\t\n",
    "\t\toutput = output.permute((1, 2, 0)).cpu()\n",
    "\t\toutput = np.array(output, np.float32)\n",
    "\t\toutput = np.round(output * 255.0)\n",
    "\n",
    "\toutput = Image.fromarray(output.astype(np.uint8))\n",
    "\n",
    "\treturn output\n",
    "\n",
    "def encode_to_tensor(image):\n",
    "    image = np.array(image, np.float32) / 255. * 2 - 1\n",
    "    image = torch.from_numpy(image).to(device)\n",
    "    image = image.permute((2, 0, 1))\n",
    "\n",
    "    return image\n",
    "\n",
    "def decode_to_image(tensor):\n",
    "    tensor = tensor.clamp(-1, 1) * 0.5 + 0.5\n",
    "    tensor = tensor.permute((1, 2, 0)).cpu()\n",
    "    tensor = np.array(tensor, np.float32)\n",
    "    tensor = np.round(tensor * 255.0)\n",
    "    tensor = Image.fromarray(tensor.astype(np.uint8))\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DehazeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.neural_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1, stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.neural_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightClearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCA_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        kernel_size = 3\n",
    "        pooling_size = 2\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=kernel_size, padding=1),#400\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(pooling_size, stride=pooling_size, padding=0),#200\n",
    "            nn.Conv2d(64, 64, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(pooling_size, stride=pooling_size, padding=0),#100\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(640_000, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 640_000),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, 100, 100)),\n",
    "            nn.Conv2d(64, 64, kernel_size=kernel_size, padding=1),#100\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=pooling_size, mode='nearest'),#200\n",
    "            nn.Conv2d(64, 64, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=pooling_size, mode='nearest'),#400\n",
    "            nn.Conv2d(64, 3, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
